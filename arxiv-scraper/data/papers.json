[
  {
    "id": "2512.25066v1",
    "title": "From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing",
    "summary": "Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.",
    "published": "2025-12-31T18:58:30Z",
    "link": "http://arxiv.org/abs/2512.25066v1",
    "authors": [
      "Xu He",
      "Haoxian Zhang",
      "Hejia Chen",
      "Changyuan Zheng",
      "Liyang Chen",
      "Songlin Tang",
      "Jiehui Huang",
      "Xiaoqiang Liu",
      "Pengfei Wan",
      "Zhiyong Wu"
    ],
    "categories": [
      "cs.CV"
    ],
    "ai_summary": "Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization.",
    "selected_date": "2026-01-03"
  },
  {
    "id": "2512.24556v1",
    "title": "Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs",
    "summary": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.",
    "published": "2025-12-31T01:40:07Z",
    "link": "http://arxiv.org/abs/2512.24556v1",
    "authors": [
      "Muhammad Abdullahi Said",
      "Muhammad Sammani Sani"
    ],
    "categories": [
      "cs.CL"
    ],
    "ai_summary": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing).",
    "selected_date": "2026-01-02"
  },
  {
    "id": "2512.24499v1",
    "title": "Training-Free Color-Aware Adversarial Diffusion Sanitization for Diffusion Stegomalware Defense at Security Gateways",
    "summary": "The rapid expansion of generative AI has normalized large-scale synthetic media creation, enabling new forms of covert communication. Recent generative steganography methods, particularly those based on diffusion models, can embed high-capacity payloads without fine-tuning or auxiliary decoders, creating significant challenges for detection and remediation. Coverless diffusion-based techniques are difficult to counter because they generate image carriers directly from secret data, enabling attackers to deliver stegomalware for command-and-control, payload staging, and data exfiltration while bypassing detectors that rely on cover-stego discrepancies. This work introduces Adversarial Diffusion Sanitization (ADS), a training-free defense for security gateways that neutralizes hidden payloads rather than detecting them. ADS employs an off-the-shelf pretrained denoiser as a differentiable proxy for diffusion-based decoders and incorporates a color-aware, quaternion-coupled update rule to reduce artifacts under strict distortion limits. Under a practical threat model and in evaluation against the state-of-the-art diffusion steganography method Pulsar, ADS drives decoder success rates to near zero with minimal perceptual impact. Results demonstrate that ADS provides a favorable security-utility trade-off compared to standard content transformations, offering an effective mitigation strategy against diffusion-driven steganography.",
    "published": "2025-12-30T22:53:33Z",
    "link": "http://arxiv.org/abs/2512.24499v1",
    "authors": [
      "Vladimir Frants",
      "Sos Agaian"
    ],
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "ai_summary": "The rapid expansion of generative AI has normalized large-scale synthetic media creation, enabling new forms of covert communication. Recent generative steganography methods, particularly those based on diffusion models, can embed high-capacity payloads without fine-tuning or auxiliary decoders, creating significant challenges for detection and remediation.",
    "selected_date": "2026-01-01"
  },
  {
    "id": "2512.23307v1",
    "title": "RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking",
    "summary": "Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.",
    "published": "2025-12-29T08:51:35Z",
    "link": "http://arxiv.org/abs/2512.23307v1",
    "authors": [
      "Jiawei Liu",
      "Zhuo Chen",
      "Rui Zhu",
      "Miaokun Chen",
      "Yuyang Gong",
      "Wei Lu",
      "Xiaofeng Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.IR"
    ],
    "ai_summary": "Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems.",
    "selected_date": "2025-12-31"
  },
  {
    "id": "2512.23705v1",
    "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
    "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
    "published": "2025-12-29T18:59:24Z",
    "link": "http://arxiv.org/abs/2512.23705v1",
    "authors": [
      "Shaocong Xu",
      "Songlin Wei",
      "Qizhe Wei",
      "Zheng Geng",
      "Hong Li",
      "Licheng Shen",
      "Qianpu Sun",
      "Shu Han",
      "Bin Ma",
      "Bohan Li",
      "Chongjie Ye",
      "Yuhang Zheng",
      "Nan Wang",
      "Saining Zhang",
      "Hao Zhao"
    ],
    "categories": [
      "cs.CV"
    ],
    "ai_summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules.",
    "selected_date": "2025-12-30"
  },
  {
    "id": "2512.20986v1",
    "title": "AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs",
    "summary": "The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.",
    "published": "2025-12-24T06:29:24Z",
    "link": "http://arxiv.org/abs/2512.20986v1",
    "authors": [
      "Yihan Wang",
      "Huanqi Yang",
      "Shantanu Pal",
      "Weitao Xu"
    ],
    "categories": [
      "cs.CR"
    ],
    "selected_date": "2025-12-29",
    "ai_summary": "The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs."
  },
  {
    "id": "2312.12345",
    "title": "Large Language Models for Automated Vulnerability Detection in Smart Contracts",
    "summary": "We present a novel approach for detecting security vulnerabilities in smart contracts using large language models. Our method achieves state-of-the-art performance on benchmark datasets, demonstrating the potential of LLMs in automated security analysis. We evaluate our approach on over 10,000 smart contracts and show significant improvements in both precision and recall compared to existing static analysis tools.",
    "published": "2024-12-15T10:30:00Z",
    "link": "http://arxiv.org/abs/2312.12345",
    "authors": [
      "Jane Smith",
      "John Doe",
      "Alice Johnson"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "selected_date": "2024-12-28"
  },
  {
    "id": "2312.54321",
    "title": "Adversarial Robustness in Vision Transformers: A Comprehensive Survey",
    "summary": "This survey provides a comprehensive overview of adversarial robustness techniques for vision transformers. We categorize existing defense mechanisms, analyze their effectiveness, and identify key challenges and future research directions. Our analysis covers both training-time and inference-time defenses, providing insights into the trade-offs between robustness and accuracy.",
    "published": "2024-12-14T14:20:00Z",
    "link": "http://arxiv.org/abs/2312.54321",
    "authors": [
      "Robert Chen",
      "Maria Garcia"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.CR"
    ],
    "selected_date": "2024-12-27"
  },
  {
    "id": "2312.98765",
    "title": "Privacy-Preserving Federated Learning with Differential Privacy Guarantees",
    "summary": "We propose a new framework for privacy-preserving federated learning that provides formal differential privacy guarantees while maintaining model utility. Our approach combines secure aggregation with adaptive noise injection, achieving better privacy-utility trade-offs than existing methods. Experiments on multiple datasets demonstrate the effectiveness of our approach in real-world scenarios.",
    "published": "2024-12-13T09:15:00Z",
    "link": "http://arxiv.org/abs/2312.98765",
    "authors": [
      "David Lee",
      "Sarah Williams",
      "Michael Brown",
      "Emma Davis"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "selected_date": "2024-12-26"
  }
]